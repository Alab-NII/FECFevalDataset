0	These results show that PRINCIPAR compares quite well with both CFG parsers.	These results indicate that the topic modeler separates topics very efficiently.	First, there was a significant difference in the variability of judges’ responses between items that were incorrectly classified and those that were correct (66% vs. 73%, respectively; two-sample t test: MATH-w-15-9-2-88, MATH-w-15-9-2-98.02).	these results show that	these results indicate that	there was a significant difference	Summary of the results	Description of the results	J95-2005_s-12-6-1-3	E17-1033_s-16-1-0-2	D13-1094_s-15-9-2-2	1
1	These results suggest that rewriting is estimated to improve translation quality.	These results show that PRINCIPAR compares quite well with both CFG parsers.	Table 1 shows topic-length statistics, Table 2 shows document statistics, and Figure 4 shows an example topic.	these results suggest that	these results show that	table * shows	Summary of the results	Reference to tables or figures	P13-4015_s-7-4-1-0	J95-2005_s-12-6-1-3	E99-1013_s-10-1-1-0	1
2	Taken together, these results are consistent with the idea that, at least for a significant portion of the data, the incorrect judgments made by both the judges and the model may have occurred on passages for which either including or omitting the connective would have been acceptable.	These results suggest that rewriting is estimated to improve translation quality.	The results with this setting are summarized in Table 5.	taken together these results	these results suggest that	are summarized in table	Summary of the results	Reference to tables or figures	D13-1094_s-15-9-2-6	P13-4015_s-7-4-1-0	D07-1033_s-12-1-4-2	0.8
3	These results indicate that the topic modeler separates topics very efficiently.	These results show that stage 2 is central to the aligner’s success.	Comparing the F1-macro with and without lexicon, it can be seen that the additional information stored in the lexicon strongly increases the score by about 20 points for English data.	these results indicate that	these results show that	comparing * it can be seen that	Summary of the results	comparison of the results	E17-1033_s-16-1-0-2	D15-1111_s-16-1-1-4	N18-1134_s-12-1-1-3	1
4	These features cause no significant difference in score (line 5).	The analysis revealed that both structural and syntactic features are important.	The hardware was tested with 1 core and 5 cores.	no significant difference	revealed that	was tested	Description of the results	The aim of the paper (again)	D09-1076_s-11-6-4-3	N12-1009_s-24-1-0-1	P15-2063_s-10-4-0-1	0.8
5	The mean score for those with extensive formal training was 90.9%.	By contrast, only a few inserted clauses were detected even if dependency structures were used.	Average values for each method are shown in Figure 1 and comparative values are shown in Figure 2.	the mean score for * was	only * were detected	are shown in	Description of the results	Reference to tables or figures	N18-1175_s-13-1-1-4	P06-2042_s-9-5-1-0	N07-2044_s-5-1-0-1	0.6
6	No statistically significant decrease in accuracy was detected in any of the algorithm/dataset combinations.	This did not affect to English results because of the spareness of “vague” links.	Table 2 summarizes the accuracy of various models on three datasets, revealing that our model offers competitive performance, both as a joint model of words and labels (Eq.	no * was detected	did not affect	table * revealing that	Description of the results	Reference to tables or figures	E12-1008_s-15-3-2-1	S10-1063_s-18-11-1-6	P18-1189_s-11-1-1-0	0.2
7	There was a significant difference in performance across different relations.	There is a significant increase between “C:±2” and “C:±1” models.	The purpose of this test was to demonstrate possible differences between WSI of different word class combinations.	there was a significant difference	a significant increase	the purpose of * was to	Description of the results	The aim of the paper (again)	S07-1082_s-7-1-0-1	P11-1139_s-17-1-0-5	E06-1018_s-6-6-0-2	1
8	No significant improvement was observed in the results when the value of MPR was greater than 5.	The MT06 result is statistically significant at MATH-w-15-6-0-22; MT08 is significant at p ≤ 0.02.	The purpose of this stage was to correct obvious mistakes.	no * was observed	significant at	the purpose of * was to	Description of the results	The aim of the paper (again)	N09-3016_s-7-3-1-1	P12-1016_s-15-6-0-1	P13-1069_s-5-6-0-4	0.8
9	The results indicate that the sciences and humanities share several topics.	Further testing revealed that accuracy outside NEs was near 99%.	The purpose of this test was to demonstrate possible differences between WSI of different word class combinations.	the results indicate that	revealed that	the purpose of * was to	Description of the results	The aim of the paper (again)	D15-1179_s-16-1-1-0	N04-2007_s-5-10-2-0	E06-1018_s-6-6-0-2	0.6
10	Accordingly, we found no noticeable bias with regard to their numbers.	There was no significant difference in learning gain between conditions.	The sixth row is interesting because Dave Toub is indeed affiliated with the Chicago Bears .	found no	no significant difference	interesting because	Description of the results	interesting or surprising results	D12-1057_s-13-8-1-4	P10-2009_s-4-1-1-1	P12-1072_s-15-8-2-7	0.8
11	No cases for which Recommendation 6 applies were found in the test corpus.	These metrics were shown to perform well for single document text summarization, especially for short summaries.	Comparing the F1-macro with and without lexicon, it can be seen that the additional information stored in the lexicon strongly increases the score by about 20 points for English data.	no * were found	were shown to	comparing * it can be seen that	Description of the results	comparison of the results	E17-3015_s-7-5-0-1	S14-1010_s-14-1-0-4	N18-1134_s-12-1-1-3	0.4
12	For the top three items, the difference between Markov (0.7968) and LSA (0.9965) was significant at the MATH-w-12-4-1-150 level.	Both results are almost the same with no significant difference.	Among them, 300 questions were used to train the Japanese answer extractor and 400 questions were used to evaluate our framework.	the difference * was significant	no significant difference	were used to	Description of the results	The aim of the paper (again)	N07-1060_s-12-4-1-4	P18-1166_s-12-1-0-2	P07-1099_s-16-1-0-4	0.75
13	Discarding bottom ranked features (up to 50%), also, did not affect the results significantly.	Different initial values were shown to affect the results of training to some extent (Table 1).	These results show that stage 2 is central to the aligner’s success.	did not affect	were shown to	these results show that	Description of the results	Summary of the results	P10-1095_s-12-3-2-1	P05-1010_s-12-1-0-4	D15-1111_s-16-1-1-4	0.6
14	There was positive correlation (MATH-w-9-1-3-33) between the F1 score for a given class and the raw number of training examples representing that class.	First, there was a significant difference in the variability of judges’ responses between items that were incorrectly classified and those that were correct (66% vs. 73%, respectively; two-sample t test: MATH-w-15-9-2-88, MATH-w-15-9-2-98.02).	As can be seen from the table, both translation directions,	there was * correlation	there was a significant difference	as can be seen from	Description of the results	Reference to tables or figures	S16-1074_s-9-1-3-1	D13-1094_s-15-9-2-2	J06-4004_s-14-8-0-1	0.8
15	Further testing revealed that accuracy outside NEs was near 99%.	The results indicate that time-aware embedding outperforms all the baselines consistently.	Surprisingly, the joint models outperform slightly, yielding a 0.4% improvement.	revealed that	the results indicate that	surprisingly	Description of the results	interesting or surprising results	N04-2007_s-5-10-2-0	D16-1260_s-9-1-3-2	D15-1040_s-15-9-2-1	0.75
16	None of the differences are statistically significant which lets us conclude that interpretability of induced descriptors is comparable for the RMN and MVPlot.	It is thus revealed that chat language is indeed dynamic.	Taken together, these results indicate that US IM , even in its automatic variant, is sensitive to semantic changes.	none of * statistically significant	revealed that	taken together these results	Description of the results	Summary of the results	D17-1200_s-12-1-2-2	P06-1125_s-19-1-2-4	N18-2020_s-10-1-3-0	0
17	The comparison between dep1-L and dep1c-L is significant at p < 0.05, and all other comparisons are significant at p < 0.0005.	The mean score for those with extensive formal training was 90.9%.	The statistics of the datasets are summarized in Table 1.	significant at	the mean score for * was	are summarized in table	Description of the results	Reference to tables or figures	P08-1068_s-10-5-0-1	N18-1175_s-13-1-1-4	D16-1171_s-10-1-0-1	0.8
18	We also notice a significant increase in similarity between Romanian and English in the modern period.	However, no significant differences were found between experiments with a different number of skips.	More surprising, however, is the mediocre performance of the WordNet lemmatizer.	a significant increase	no * were found	more surprising	Description of the results	interesting or surprising results	D14-1112_s-14-1-1-7	S14-2048_s-8-1-2-3	Q16-1021_s-14-4-1-0	1
19	On the other hand, additive models remain unaffected by these changes.	Further testing revealed that accuracy outside NEs was near 99%.	Table 10 shows results using unigrams, while Table 10 shows results for primary ciphers generated from bigrams.	unaffected by	revealed that	table * shows	Description of the results	Reference to tables or figures	P18-1012_s-12-1-0-1	N04-2007_s-5-10-2-0	D16-1141_s-15-1-0-1	0.6
20	Where suitable evidence was found, the RTE component incorrectly classified 13.84% (MATH-w-22-1-0-117) of claims.	The mean score for those with extensive formal training was 90.9%.	Trigram models were used to compute the test word perplexity.	evidence was found	the mean score for * was	were used to	Description of the results	The aim of the paper (again)	N18-1074_s-22-1-0-3	N18-1175_s-13-1-1-4	J09-1002_s-17-4-0-2	1
21	Further analysis showed that its bad performance is due to our choice of confounders as highly similar lemmas (cf. Section 2).	We found no significant improvement when the alternative was used.	Table 3 shows the BLEU scores, and Table 4 shows the precision.	further analysis showed that	found no	table * shows	Description of the results	Reference to tables or figures	N18-2033_s-5-1-1-2	N04-1010_s-8-17-2-3	N06-1001_s-14-1-0-1	0.8
22	We allowed our model to tune on the gold data, which surprisingly did not increase performance particularly much.	No significant interaction effect was found for module by group.	Surprisingly, GUSPEE even surpassed the supervised upper bound of MultiR.	did not increase	no * was found	surprisingly	Description of the results	interesting or surprising results	S17-2021_s-12-1-0-4	P16-4021_s-6-1-1-7	N15-1077_s-15-1-1-2	1
23	No significant interaction effect was found for module by group.	None of the differences are statistically significant which lets us conclude that interpretability of induced descriptors is comparable for the RMN and MVPlot.	When comparing the two online learning models, it can be seen that MIRA outperforms the averaged perceptron method.	no * was found	none of * statistically significant	comparing * it can be seen that	Description of the results	comparison of the results	P16-4021_s-6-1-1-7	D17-1200_s-12-1-2-2	P05-1012_s-10-5-1-6	0.75
24	However, there was no evidence that the tailored letters were any better than the non-tailored ones in terms of increasing cessation rates.	(The other 3 entrainment variables did not show significant relationships with participation.)	The validation set was used to estimate performance during algorithm development and the test set was used to generate the final results.	there was no evidence	did not show	was used to	Description of the results	The aim of the paper (again)	P01-1057_s-6-10-0-1	D16-1149_s-21-8-2-1	D12-1069_s-19-1-1-6	0.8
25	In the case of the LDA model, only 23 bursty topics were detected.	The mean score for those with extensive formal training was 90.9%.	These results indicate that our MGL generates responses with higher quality.	only * were detected	the mean score for * was	these results indicate that	Description of the results	Summary of the results	P12-1056_s-11-1-0-3	N18-1175_s-13-1-1-4	P18-1137_s-14-5-0-5	0.8
26	(The other 3 entrainment variables did not show significant relationships with participation.)	The results indicate that the sciences and humanities share several topics.	The bottom half of the table shows cross-domain experiments for Sieve using the lexical heuristic at the end of its rule set (LexEnd).	did not show	the results indicate that	the bottom half of the table	Description of the results	Reference to tables or figures	D16-1149_s-21-8-2-1	D15-1179_s-16-1-1-0	P13-2015_s-10-6-1-1	1
27	Different initial values were shown to affect the results of training to some extent (Table 1).	Both results are almost the same with no significant difference.	Surprisingly, the joint models outperform slightly, yielding a 0.4% improvement.	were shown to	no significant difference	surprisingly	Description of the results	interesting or surprising results	P05-1010_s-12-1-0-4	P18-1166_s-12-1-0-2	D15-1040_s-15-9-2-1	0.5
28	This is indeed the case, as can be seen from Figure 6.	Results are shown in the last row of Table 4 and precision-recall curves are shown in Figure 5.	Comparing the F1-macro with and without lexicon, it can be seen that the additional information stored in the lexicon strongly increases the score by about 20 points for English data.	as can be seen from	are shown in	comparing * it can be seen that	Reference to tables or figures	comparison of the results	P11-1028_s-10-8-2-3	N10-1119_s-9-17-1-1	N18-1134_s-12-1-1-3	1
29	The results are presented in Tables 5, 6, and 7.	Average values for each method are shown in Figure 1 and comparative values are shown in Figure 2.	These results show that PRINCIPAR compares quite well with both CFG parsers.	are presented in	are shown in	these results show that	Reference to tables or figures	Summary of the results	D17-1204_s-13-1-0-0	N07-2044_s-5-1-0-1	J95-2005_s-12-6-1-3	1
30	Table 1 shows topic-length statistics, Table 2 shows document statistics, and Figure 4 shows an example topic.	It is apparent from Table 2 that unigram features yield curiously high performance in many genres.	These results indicate that leveraging unlabeled data yields significant improvements.	table * shows	it is apparent from	these results indicate that	Reference to tables or figures	Summary of the results	E99-1013_s-10-1-1-0	D13-1181_s-8-1-0-0	S14-2120_s-13-7-3-1	1
31	This is not always true, as shown in Experiment I.	Figure 8 provides an overview of the architecture of MOOSE.	This is interesting because these two SE-types constitute the broader SE category of generalizing statives.	as shown in	figure * provides	interesting because	Reference to tables or figures	interesting or surprising results	D07-1066_s-11-1-0-5	J98-3003_s-13-1-0-0	P07-1113_s-20-1-1-1	1
32	It is apparent from Table 2 that unigram features yield curiously high performance in many genres.	The significance levels of the improvements over Local are highlighted in Table 4.	These results indicate that our MGL generates responses with higher quality.	it is apparent from	highlighted in table	these results indicate that	Reference to tables or figures	Summary of the results	D13-1181_s-8-1-0-0	D09-1018_s-13-4-2-5	P18-1137_s-14-5-0-5	1
33	Table 1 compares different transducers on an English test case.	Figure 5 provides a more detailed characterization of LNQ’s performance.	When comparing the two online learning models, it can be seen that MIRA outperforms the averaged perceptron method.	table * compares	figure * provides	comparing * it can be seen that	Reference to tables or figures	comparison of the results	E97-1059_s-10-1-2-0	P18-1029_s-12-6-1-0	P05-1012_s-10-5-1-6	1
34	Average values for each method are shown in Figure 1 and comparative values are shown in Figure 2.	The statistics for this data are presented in Table 1.	Where suitable evidence was found, the RTE component incorrectly classified 13.84% (MATH-w-22-1-0-117) of claims.	are shown in	are presented in	evidence was found	Reference to tables or figures	Description of the results	N07-2044_s-5-1-0-1	D08-1023_s-8-1-0-1	N18-1074_s-22-1-0-3	1
35	Statistics of the data sets are summarized in Table 3.	The top half of the table shows the results on the English evaluation set.	The validation set was used to estimate performance during algorithm development, while the test set was used to generate the final experimental results.	are summarized in table	the top half of the table	was used to	Reference to tables or figures	The aim of the paper (again)	D09-1114_s-7-1-0-5	P17-1009_s-23-1-0-1	D12-1069_s-17-1-0-6	1
36	The significance levels of the improvements over Local are highlighted in Table 4.	Table 10 shows results using unigrams, while Table 10 shows results for primary ciphers generated from bigrams.	There was a significant difference in performance across different relations.	highlighted in table	table * shows	there was a significant difference	Reference to tables or figures	Description of the results	D09-1018_s-13-4-2-5	D16-1141_s-15-1-0-1	S07-1082_s-7-1-0-1	1
37	The top half of the table shows the results on the English evaluation set.	This is highlighted in Table 2, where row “B-opt-F” contains the best results optimizing the entity F-measure (at MATH-w-12-11-1-73), row “B-opt-AV” contains the best results optimizing ACE-Value (at MATH-w-12-11-1-93), and the last line “Twin-model” contains the results of the proposed twin-model.	However, there was no evidence that the tailored letters were any better than the non-tailored ones in terms of increasing cessation rates.	the top half of the table	highlighted in table	there was no evidence	Reference to tables or figures	Description of the results	P17-1009_s-23-1-0-1	N07-1010_s-12-11-1-1	P01-1057_s-6-10-0-1	1
38	It can be seen from Table 7 that the lexical and gazetteer related features are helpful.	Figure 8 provides an overview of the architecture of MOOSE.	The purpose of creating this subset model was to simulate a resource-poor language.	it can be seen from	figure * provides	the purpose of * was to	Reference to tables or figures	The aim of the paper (again)	P11-1037_s-21-1-0-3	J98-3003_s-13-1-0-0	D09-1040_s-7-1-0-4	1
39	The bottom half of the table shows cross-domain experiments for Sieve using the lexical heuristic at the end of its rule set (LexEnd).	We see from this table that the results are similar across the two tasks.	Interestingly, all updates with Laplacian prior resulted in low performance.	the bottom half of the table	from this table	interestingly	Reference to tables or figures	interesting or surprising results	P13-2015_s-10-6-1-1	P12-1063_s-19-1-0-3	P07-3009_s-11-8-0-2	1
40	Table 2 summarizes the accuracy of various models on three datasets, revealing that our model offers competitive performance, both as a joint model of words and labels (Eq.	Figure 4 provides a rough sketch of the room layout.	By comparing the tagging accuracy for all words in tables 2 and 3, it can be seen that the accuracy had be underestimated by 0.13-0.18 percentage points.	table * revealing that	figure * provides	comparing * it can be seen that	Reference to tables or figures	comparison of the results	P18-1189_s-11-1-1-0	J97-1006_s-14-1-0-0	E09-1060_s-12-1-4-2	0.5
41	The table also illustrates three baseline results on the test data only.	This is indeed the case, as can be seen from Figure 6.	The grammatical channel was tested in the SILC translation system.	the table * illustrates	as can be seen from	was tested	Reference to tables or figures	The aim of the paper (again)	D08-1030_s-13-3-1-3	P11-1028_s-10-8-2-3	P98-2230_s-10-1-0-0	1
42	Figure 4 provides a rough sketch of the room layout.	The Table 6 illustrates examples of polar phrases and their polarity values.	Comparing the F1-macro with and without lexicon, it can be seen that the additional information stored in the lexicon strongly increases the score by about 20 points for English data.	figure * provides	the table * illustrates	comparing * it can be seen that	Reference to tables or figures	comparison of the results	J97-1006_s-14-1-0-0	D07-1115_s-16-7-3-0	N18-1134_s-12-1-1-3	1
43	Table 1 gives some examples, and Table 2 presents some statistics.	The second half of the table illustrates Cannot-link tuple rules.	Taken together these results indicate that the ILP models perform a fair amount of rewriting without simply rehashing the source sentence.	table * presents	the table * illustrates	taken together these results	Reference to tables or figures	Summary of the results	N07-2024_s-5-1-0-3	D13-1040_s-10-6-2-5	D11-1038_s-8-1-1-5	1
44	We see from this table that the results are similar across the two tasks.	Figure 4 provides a rough sketch of the room layout.	However, there was no evidence that the tailored letters were any better than the non-tailored ones in terms of increasing cessation rates.	from this table	figure * provides	there was no evidence	Reference to tables or figures	Description of the results	P12-1063_s-19-1-0-3	J97-1006_s-14-1-0-0	P01-1057_s-6-10-0-1	1
45	By comparing the tagging accuracy for all words in tables 2 and 3, it can be seen that the accuracy had be underestimated by 0.13-0.18 percentage points.	By comparing the tagging accuracy for all words in tables 2 and 3, it can be seen that the accuracy had be underestimated by 0.13-0.18 percentage points.	On the other hand, additive models remain unaffected by these changes.	comparing * it can be seen that	comparing * it can be seen that	unaffected by	comparison of the results	Description of the results	E09-1060_s-12-1-4-2	E09-1060_s-12-1-4-2	P18-1012_s-12-1-0-1	1
46	'Four human judges were used to determine the ''gold standard''.'	The algorithm above was tested in the SILC translation system.	These results indicate that leveraging unlabeled data yields significant improvements.	were used to	was tested	these results indicate that	The aim of the paper (again)	Summary of the results	A00-2034_s-4-1-1-3	P96-1021_s-4-1-0-0	S14-2120_s-13-7-3-1	1
47	The extracted 10 words were compared with the manually labeled keywords.	Our experiments aimed to answer three questions: Which event chains are worth keeping?	Taken together, these results suggest that model combination contributes to the success of both models, but to a larger extent for RG.	were compared	aimed to	taken together these results	The aim of the paper (again)	Summary of the results	P07-1070_s-18-1-1-3	E12-1034_s-8-1-0-0	P17-2025_s-7-4-2-1	0.8
48	We aimed to include as many different domains as possible annotated under compatible schemes.	The grammatical channel was tested in the SILC translation system.	Taken together, these results indicate that the frequency of the construction is more important than the size of the training set for this type of transformation.	aimed to	was tested	taken together these results	The aim of the paper (again)	Summary of the results	N10-1004_s-10-1-0-0	P98-2230_s-10-1-0-0	P07-1122_s-14-1-1-3	0.75
49	The algorithm above was tested in the SILC translation system.	The purpose of this evaluation was to determine if any patterns consistently produce poor questions.	It is apparent from Table 2 that unigram features yield curiously high performance in many genres.	was tested	the purpose of * was to	it is apparent from	The aim of the paper (again)	Reference to tables or figures	P96-1021_s-4-1-0-0	P14-2053_s-9-1-0-1	D13-1181_s-8-1-0-0	0.8
50	The purpose of this evaluation was to determine if any patterns consistently produce poor questions.	We aimed to extract facts of the 92 most frequent relations in Freebase 2009.	These metrics were shown to perform well for single document text summarization, especially for short summaries.	the purpose of * was to	aimed to	were shown to	The aim of the paper (again)	Description of the results	P14-2053_s-9-1-0-1	P13-2141_s-9-1-0-0	S14-1010_s-14-1-0-4	1
51	The validation set was used to estimate performance during algorithm development, while the test set was used to generate the final experimental results.	Our experiments aimed to answer three questions: Which event chains are worth keeping?	Taken together, these results are consistent with the idea that, at least for a significant portion of the data, the incorrect judgments made by both the judges and the model may have occurred on passages for which either including or omitting the connective would have been acceptable.	was used to	aimed to	taken together these results	The aim of the paper (again)	Summary of the results	D12-1069_s-17-1-0-6	E12-1034_s-8-1-0-0	D13-1094_s-15-9-2-6	1
52	The sixth row is interesting because Dave Toub is indeed affiliated with the Chicago Bears .	The most surprising finding is that the best performance was achieved by the unlexicalized PCFG baseline model.	No cases for which Recommendation 6 applies were found in the test corpus.	interesting because	the most surprising	no * were found	interesting or surprising results	Description of the results	P12-1072_s-15-8-2-7	P03-1013_s-13-1-2-0	E17-3015_s-7-5-0-1	1
53	Surprisingly, the joint models outperform slightly, yielding a 0.4% improvement.	Although this might be counterintuitive, given that WCNs improve the slot accuracy overall.	When comparing the two online learning models, it can be seen that MIRA outperforms the averaged perceptron method.	surprisingly	counterintuitive	comparing * it can be seen that	interesting or surprising results	comparison of the results	D15-1040_s-15-9-2-1	E09-1028_s-11-11-3-4	P05-1012_s-10-5-1-6	1
54	The results for the more challenging English-Chinese pair were more surprising.	Surprisingly, GUSPEE even surpassed the supervised upper bound of MultiR.	AUC scores are largely unaffected by change in label balance.	more surprising	surprisingly	unaffected by	interesting or surprising results	Description of the results	D15-1128_s-10-1-0-3	N15-1077_s-15-1-1-2	P16-3016_s-7-1-0-0	1
55	Interestingly, all updates with Laplacian prior resulted in low performance.	Although this might be counterintuitive, given that WCNs improve the slot accuracy overall.	Comparing the F1-macro with and without lexicon, it can be seen that the additional information stored in the lexicon strongly increases the score by about 20 points for English data.	interestingly	counterintuitive	comparing * it can be seen that	interesting or surprising results	comparison of the results	P07-3009_s-11-8-0-2	E09-1028_s-11-11-3-4	N18-1134_s-12-1-1-3	1
56	One of the most surprising results of our experiments is that the object of the action alone is a very effective feature, achieving 89.04%.	Although this might be counterintuitive, given that WCNs improve the slot accuracy overall.	These results show that the LexHMM is modelling ambiguity classes as intended.	the most surprising	counterintuitive	these results show that	interesting or surprising results	Summary of the results	S14-1008_s-9-1-1-4	E09-1028_s-11-11-3-4	E14-1013_s-10-4-2-2	1
57	This is very counterintuitive, and we are still trying hard to find its real cause.	Interestingly, ICA and ILP improve over Local in different ways.	These results show that stage 2 is central to the aligner’s success.	counterintuitive	interestingly	these results show that	interesting or surprising results	Summary of the results	S18-1176_s-17-1-0-5	D09-1018_s-13-4-3-0	D15-1111_s-16-1-1-4	1
58	The most striking fact about this graph is the tremendous efficiency of the extension model.	The results for the more challenging English-Chinese pair were more surprising.	These results indicate that our MGL generates responses with higher quality.	the most striking	more surprising	these results indicate that	interesting or surprising results	Summary of the results	P95-1030_s-8-6-0-1	D15-1128_s-10-1-0-3	P18-1137_s-14-5-0-5	1
