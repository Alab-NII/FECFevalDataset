discussion	Background provided by past work	prior work that has	In contrast to prior work that has assumed that the sequenceof-character information captured by character ngrams is sufficient, position embeddings also capture sequenceof-ngram information.
discussion	Background provided by past work	prior work that has	Unlike prior work that has focused on inferring “life trajectories” from categorical survey data, we learn relevant structure in an unsupervised manner directly from text, opening the door to applying this method to a broad set of biographies beyond Wikipedia (including full-text books from the Internet Archive or Hathi Trust, and other encyclopedic biographies as well).
discussion	Background provided by past work	previous studies	World knowledge, especially obtained from Wikipedia, has shown to be useful in previous studies.
discussion	Background provided by past work	previous studies	These results show the same tendency as the previous studies discussed in Section 3.
discussion	Background provided by past work	previous studies	The results challenge the reasons given in previous studies for rejecting the rule-based model.
discussion	Background provided by past work	has been reported	This finding is at odds with what has been reported for parsing models trained on the Penn Treebank.
discussion	Background provided by past work	has been reported	It has been reported that syllabification can potentially improve pronunciation performance in English (CITE-p-11-1-10).
discussion	Background provided by past work	has been reported	To the best of our knowledge no other documented evidence has been reported for Sinhala grapheme-to-phoneme conversion in the literature.
discussion	Background provided by past work	has been reported	Despite having no parameters at all, it has been reported to perform well in many collaborative filtering tasks (CITE-p-20-1-9).
discussion	Restatement of the results	interesting finding is	The most interesting finding is that these gains are clearly observed on out-of-domain tests.
discussion	Restatement of the results	interesting finding is	The most interesting finding is that performance is stable when 30 or more search results are considered.
discussion	Restatement of the results	interesting finding is	An interesting finding is that our approach performs better than the baseline even they use the same phrase pairs.
discussion	Restatement of the results	interesting finding is	The most interesting finding is the poor performance of the metrics based on word probability distributions previously used for this task.
discussion	Restatement of the results	interesting finding is	The most interesting finding is that these dominance relations encode sufficient information to enable the derivation of discourse structures that are almost indistinguishable from those built by human annotators.
discussion	Restatement of the results	the most interesting finding is	The most interesting finding is that these gains are clearly observed on out-of-domain tests.
discussion	Restatement of the results	the most interesting finding is	The most interesting finding is that performance is stable when 30 or more search results are considered.
discussion	Restatement of the results	the most interesting finding is	The most interesting finding is the poor performance of the metrics based on word probability distributions previously used for this task.
discussion	Restatement of the results	the most interesting finding is	The most interesting finding is that these dominance relations encode sufficient information to enable the derivation of discourse structures that are almost indistinguishable from those built by human annotators.
discussion	Restatement of the results	the most interesting finding is	The most interesting finding is that measures of domain specificity perform unexpectedly low for bigram and trigram recognition but when being applied to their unigram components they appear in the upper parts of the tree.
discussion	Restatement of the results	was found to	For these pairs, the monotone search was found to be sufficient.
discussion	Restatement of the results	was found to	This reranking was found to result in a minor performance gain.
discussion	Restatement of the results	was found to	The identification accuracy was found to be as high as approximately 36%.
discussion	Restatement of the results	was found to	In addition, the proposed measure was found to be relatively robust to ASR errors.
discussion	Restatement of the results	was found to	In our experiments, the distance feature was found to be less important compared to the embedding features.
discussion	Restatement of the results	the results of this study	The results of this study show that applying NLP techniques to team discourse can provide accurate predictions of performance.
discussion	Restatement of the results	experiments did not	Indeed, our experiments did not find a persistent advantage to observing stress over modeling syllable weight.
discussion	Restatement of the results	experiments did not	Although based on much more labeled training data, these experiments did not yield significantly better results.
discussion	Restatement of the results	experiments did not	For the classes PPER PPOS, NE and All, our Co-Training experiments did not yield any benefits worth reporting.
discussion	Restatement of the results	experiments did not	Our preliminary experiments did not show noticeable improvements from bigram or character-based features, but it is possible that higher-level features such as morphological, part-of-speech or syntactic features could yield further performance gains.
discussion	Restatement of the results	experiments did not	While our experiments did not reveal any serious shortcomings (unlike those of Mathet et al., 2012 who in the case of categorisation showed that for large MATH-w-11-1-0-72 the uncorrected measure can be increasing), the methodological problems of uncorrected metrics makes us wary of LAS as an agreement metric.
discussion	Restatement of the results	the most important finding	The most important finding is that our distributional neural network model is very effective in establishing similarity matching between clues.
discussion	Restatement of the results	the most important finding	The most important finding of this study is that, even on a test painstakingly designed to exclusively assess composition, vector addition matches or outperforms sophisticated CDSM.
discussion	Unexpected outcome	surprisingly	Also, the impact of imperfect temporal data is surprisingly minimal.
discussion	Unexpected outcome	surprisingly	Our approach is surprisingly effective in learning from free-form language.
discussion	Unexpected outcome	surprisingly	Word embeddings are surprisingly variable, even for relatively high frequency words.
discussion	Unexpected outcome	surprisingly	Surprisingly, this natural approach does not work, even for simple examples.
discussion	Unexpected outcome	what is surprising	What is surprising is that the judgments of the group, aggregated in this way, correspond more closely to expert opinion than (in many cases) the best individual annotators.
discussion	Unexpected outcome	what is surprising	What is surprising about the comparative performance of morphological and distributional models is that there is no language for which the distributional model outperforms the morphological model by a wide margin.
discussion	Unexpected outcome	what is surprising	What is surprising, moreover, is that diversity-enabled ProbDTs are superior in performance to their non-diversity counterparts (with a notable exception for SSDT at MATH-w-11-1-2-88), which suggests that selecting marginal sentences is an important part of generating a summary.
discussion	Unexpected outcome	was unexpected	This was unexpected but found to be due to instances in which there are short distances between the pronoun and verb in English.
discussion	Unexpected outcome	was unexpected	It was unexpected for verb disambiguation results to be as strong as nouns because a previous study using Web Selectors found noun sense disambiguation clearly stronger than verb sense disambiguation on a coarse-grained corpus (CITE-p-8-1-10).
discussion	Unexpected outcome	it is somewhat surprising that	It is somewhat surprising that we did not achieve reliable performance gains which were seen in the related work described above.
discussion	Unexpected outcome	it is somewhat surprising that	On the other hand, it is somewhat surprising that the three information-status classifiers have yielded coreference systems that perform at essentially the same level of performance.
discussion	Unexpected outcome	it is somewhat surprising that	Thus it is somewhat surprising that we show a translated English treebank may help Chinese parsing, as English and Chinese even belong to two different language systems.
discussion	Unexpected outcome	contrary to expectations	We show that, contrary to expectations, the deletion of non-subsective adjectives from a sentence does not necessarily result in non-entailment.
discussion	Unexpected outcome	contrary to expectations	Also contrary to expectations, the error analysis suggests that there has been no improvement in either the handing of unknown words, nor prepositional phrases.
discussion	comparison of the results and past work	this study confirms	More generally, this study confirms that insights drawn from the field of theoretical Translation Studies, namely, the dual claim according to which translations as such differ from originals, and translations from different source languages differ from each other, can be verified experimentally and contribute to the performance of machine translation.
discussion	comparison of the results and past work	this study confirms	More generally, this study confirms that insights drawn from the field of theoretical translation studies, namely the dual claim according to which (1) translations as such differ from originals, and (2) translations from different source languages differ from each other, can be verified experimentally and contribute to the performance of machine translation.
discussion	comparison of the results and past work	also reported	Empirically, CITE-p-19-5-3 also reported exact-search segmenter performing worse than beam-search segmenters.
discussion	comparison of the results and past work	also reported	CITE-p-27-3-4 also reported high accuracy (95.8 UAS and 94.6 LAS) by using a converter.
discussion	comparison of the results and past work	is consistent with	This is consistent with intuition from political observers that the public supports veterans.
discussion	comparison of the results and past work	is consistent with	This observation is consistent with earlier findings by Purandare and Pedersen for general English text.
discussion	comparison of the results and past work	accords with	This accords with an understated result of Boston et al.’s eye-tracking study (2008a): a richer language model predicts eye movements during reading better than an oversimplified one.
discussion	comparison of the results and past work	corroborates	This work further corroborates Kuhlmann’s work on Czech (PDT) for Hindi (CITE-p-18-3-5).
discussion	comparison of the results and past work	corroborates	The worse results on biomedical scientific papers from a different source also corroborates our finding that hedge cues can be highly ambiguous.
discussion	comparison of the results and past work	corroborates	This conclusion contradicts the assumptions of the structuro-cognitivist paradigm, but corroborates CITE-p-20-1-8’s view that the information carried by semantic predictors is largely correlated with that of lexico-syntactical ones.
discussion	comparison of the results and past work	these results corroborate	Overall, these results corroborate previous studies suggesting that highly precise sense annotations can be obtained by leveraging multiple languages (CITE-p-15-1-17, CITE-p-15-1-18).
discussion	comparison of the results and past work	in accordance with * cite-	Regarding the set of word embeddings, we found that higher-dimensional embeddings provide better performance, which is in accordance with what CITE-p-16-1-15 found.
discussion	comparison of the results and past work	in accordance with * cite-	As we have seen, training on source language x and testing on source language y provides us with a good estimation of the distance between languages, in accordance with what we find in standard works on typology (cf. CITE-p-16-1-7).
discussion	comparison of the results and past work	are consistent with * cite-	These findings are consistent with a recent study that suggested θ should be closer to 2 (CITE-p-16-1-2).
discussion	comparison of the results and past work	are consistent with * cite-	Both of these findings are consistent with the observations about human variation in the selection of referential forms, as discussed by CITE-p-21-3-2.
discussion	comparison of the results and past work	are consistent with * cite-	These results are consistent with the findings in (CITE-p-19-1-1) that the appositive and copula structures are indicative to find the is-a relation.
discussion	comparison of the results and past work	are consistent with * cite-	We can simply mention that the errors we observed are consistent with previous systems based on Harris's hypothesis (see (CITE-p-8-1-9) and CITE-p-8-1-7 for a longer discussion).
discussion	comparison of the results and past work	are consistent with * cite-	The results also show that the Boolean weighting method outperforms the frequency weighting method in terms of average MATH-w-13-6-0-140 -measure by over 1.25%, which are consistent with the previous research by CITE-p-17-1-35.
discussion	comparison of the results and past work	are in line with	The results obtained are in line with those obtained by the other participants and they encourage us to continue working on this problem.
discussion	comparison of the results and past work	are in line with	The precision scores are in line with the scores reported by CITE-p-20-3-11 in a similar experiment discussed under related work.
discussion	comparison of the results and past work	in contrast to earlier	In contrast to earlier approaches, our model incorporates detailed syntactic information.
discussion	comparison of the results and past work	in contrast to earlier	In contrast to earlier studies, our basic method showed a significantly improved communication rate over no prediction.
discussion	comparison of the results and past work	in contrast to earlier	In contrast to earlier work, these results were achieved without manual labelling of the corpus, and without direct manual selection of high performance patterns.
discussion	comparison of the results and past work	in contrast to earlier	This is in contrast to earlier studies (CITE-p-32-3-14, CITE-p-32-1-5) where pretrained embeddings usually improved model performances by 3-4%.
discussion	comparison of the results and past work	than that of * previous	Not only is our parser more accurate, but the learned grammar is also significantly smaller than that of previous work.
discussion	comparison of the results and past work	than that of * previous	Our proposed MATH-w-7-4-0-20 kernel obtains an F-score that is higher than that of the previous state of the art.
discussion	comparison of the results and past work	than that of * previous	On the English→German task, the best performance of 21.59 BLEU by our model is higher than that of the previous state of the art (20.67) reported in (CITE-p-20-4-0).
discussion	comparison of the results and past work	than that of * previous	For the subject-verb-object phrase task, the result is achieved without any pre-trained word vectors using a corpus an order of magnitude smaller than that of the previous state of the art.
discussion	comparison of the results and past work	than that of * previous	10 Note that DBM-1’s 39% average accuracy with standard training (see Table 2) was already nearly a full point higher than that of any single previous best system (SCAJ 6 — see Table 8).
discussion	Explanation for the findings	a possible explanation for	A possible explanation for this phenomenon could lie in the number of translations the systems have to choose from.
discussion	Explanation for the findings	a possible explanation for	A possible explanation for that is polysemous adjectives do not present a homogeneous behaviour: ambiguous adjectives such as irônic 'ironic' are
discussion	Explanation for the findings	a possible explanation for	4 A possible explanation for these disparate findings is a systematic variation of annotator numbers employed in the domains (see Table 5).
discussion	Explanation for the findings	a possible explanation for	A possible explanation for this is that the SE07 dataset appears to be somewhere in the middle between EL09 and GE15 in terms of substitute variance.
discussion	Explanation for the findings	a possible explanation for	A possible explanation for the superior performance of the multi-label approach is that the overlap of the bidirectional entailment with forward and backward entailment might confuse the multi-class learner.
discussion	Explanation for the findings	may be explained by	This may be explained by domain differences in training and evaluation data.
discussion	Explanation for the findings	may be explained by	The relatively low number of results for Russian may be explained by weaknesses of langid.py with deviations of encoding standards.
discussion	Explanation for the findings	may be explained by	Alternatively, the absence of correlations may be explained by the fact that not all perception necessarily leads to a change in production, as CITE-p-16-1-11 found.
discussion	Explanation for the findings	may be explained by	This may be explained by the fact that different authors aimed to identify a different scope of linguistic phenomena and thus interpreted the concept of “light verb construction” slightly differently.
discussion	Explanation for the findings	may be explained by	The results we obtained on the test set were quite different from the results on the development set, which may be explained by the small size of the training data set.
discussion	Explanation for the findings	can be explained by	This can be explained by the differing training conditions (as previously mentioned).
discussion	Explanation for the findings	can be explained by	We find that this difference can be explained by the different coverage of the paraphrasing resources.
discussion	Explanation for the findings	can be explained by	This can be explained by fact that immediately neighboring sentences are more likely to be related.
discussion	Explanation for the findings	can be explained by	This can be explained by the fact that they often lack proper context in an extractive summary.
discussion	Explanation for the findings	can be explained by	This can be explained by the fact that the translation objective deteriorates the generalization capabilities of word embeddings.
discussion	Explanation for the findings	may explain	This may explain why EAT features outperformed USF in the validation tests.
discussion	Explanation for the findings	may explain	This may explain the continuing popularity of single-solution pipeline architectures in applied NLG systems.
discussion	Explanation for the findings	may explain	This may explain why only two of the top seven word association features use min values.
discussion	Explanation for the findings	may explain	This may explain why our system outperforms other approaches that use a similar set of overlap features.
discussion	Explanation for the findings	may explain	This may explain why we score higher, in absolute terms, on that dataset than on the more abstract one.
discussion	Explanation for the findings	may be due to	This may be due to the distribution of categories among the corpora.
discussion	Explanation for the findings	may be due to	It may be due to that we used a much simpler feature set.
discussion	Explanation for the findings	may be due to	Finally, we could also find cases that may be due to annotation errors.
discussion	Explanation for the findings	may be due to	Both of these results however may be due to limitations of the current method.
discussion	Explanation for the findings	may be due to	This may be due to the difference in perplexity of the our Evaluator-SLM.
discussion	Explanation for the findings	results are likely to	The higher the number of annotators who are able to agree, the less bias and distortion can be expected in the data and the more stable and comparable the results are likely to be.
discussion	Explanation for the findings	could be attributed to	The bigger improvement for Indonesian could be attributed to the regular patterns of orthographic changes which appear on the segmentation boundaries.
discussion	Explanation for the findings	could be attributed to	As shown in Figure 3, whether or not contributors could be attributed to the hearer did not correlate with the choice of SINCEor BECAUSE.
discussion	Explanation for the findings	could be attributed to	As for the time entity identification task, the performance differences between development and test dataset could be attributed to the annotation distributions of the datasets.
discussion	Explanation for the findings	could be attributed to	This performance difference (around 15%) could be attributed to the additional features (see Table 1) that the state-of-the-art model added to their system.
discussion	Explanation for the findings	could be attributed to	Justification for this discrepancy could be attributed to the differences in the data set, but there is also a possibility that ambiguity in the description of the features led to improper extraction techniques.
discussion	Explanation for the findings	it may be that	But it may be that something more subtle is happening.
discussion	Explanation for the findings	it may be that	It may be that linearization of the dependency graph removes too much of its information.
discussion	Explanation for the findings	it may be that	It may be that users have more familiarity with the concept of speech input than handwriting.
discussion	Explanation for the findings	it may be that	However, it may be that human language processing can be factored into separate processes of comprehension and reasoning.
discussion	Explanation for the findings	it may be that	It may be that news article style makes the lead-3 baseline very strong with respect to any metric.
discussion	Explanation for the findings	the reason for this is	We think the reason for this is because a comprehensive summary was made by latent topics.
discussion	Explanation for the findings	the reason for this is	The reason for this is that the weighting method depends on all query terms and all of the thesauri.
discussion	Explanation for the findings	the reason for this is	We hypothesize that the reason for this is the noise and the limited size of the collected training data.
discussion	Explanation for the findings	the reason for this is	The reason for this is that a completeness proof along the above lines runs into problems for such extended logics.
discussion	Explanation for the findings	the reason for this is	We think that the reason for this is that our feature set for Basque is better, although our ML algorithm is worse.
discussion	Explanation for the findings	might be explained	7 A closer look at the data revealed that this might be explained by some inconsistencies between annotations.
discussion	Explanation for the findings	might be explained	This might be explained by the fact that WMT was annotated by only one annotator, while the SNLI corpus was annotated by many.
discussion	Explanation for the findings	might be explained	This might be explained by the fact that SMS messages tend to be more direct, whereas the same tweet can express, or show signs of, contradictory sentiments.
discussion	Explanation for the findings	might be explained	This phenomenon might be explained by the observation that “the propensity to post online reviews is higher for movies that are perceived by consumers to be exceptionally good or exceptionally bad” (CITE-p-11-1-1).
discussion	Explanation for the findings	might be explained	Taking the union of all techniques does not yield additional gains: this might be explained by the fact that incorrect predictions are proportionnally more present and consequently have a greater impact when combining techniques without weighting them, possibly at the level of each prediction.
discussion	Explanation for the findings	may be limited	This suggest that improvement from ensembles may be limited for this challenging problem, and new perspectives are necessary.
discussion	Explanation for the findings	may be limited	The thresholding here may be limited by the resources, and a corpus using a larger word count may yield an improved result.
discussion	Explanation for the findings	may be limited	This suggests that the distributional hypothesis may not be equally applicable to all types of semantic information, and in particular, it may be limited with respect to attributive properties.
discussion	Explanation for the findings	may be limited	However, the results suggest that the proposed method offers a more affordable approach that provides reasonable coverage and quality, even if individual general knowledge sources may be limited in themselves.
discussion	Explanation for the findings	it could be argued that	It could be argued that the recognition incentive was motivating players in the free condition and thus some incentive was required.
discussion	Explanation for the findings	it could be argued that	It could be argued that the discourse chunk information, being based on tags, gives the DA tagger extra information about the tags themselves, and thus gives an unfair ÔboostÕ to the performance.
discussion	Explanation for the findings	it could be argued that	While it could be argued that our language identification accuracy of 84% is too low to be useful here, we believe that the principal reason for this performance decrease is the reduction in the amount of data available for the training of the separate models.
discussion	Explanation for the findings	need to be interpreted	Of course, our results need to be interpreted with prudence, since the automatic classifiers for detecting SOEs and CF on SOEs in the whole dataset are far from perfect.
discussion	Explanation for the findings	need to be interpreted	Nevertheless, we assume that the times for these phases is mostly a constant for a given problem category, and hence the results presented in this paper need to be interpreted on a per problem category basis.
discussion	Explanation for the findings	should be interpreted	However, results should be interpreted in light of the following two factors.
discussion	Explanation for the findings	should be interpreted	We would like to emphasize that these comparisons should be interpreted with caution.
discussion	Explanation for the findings	should be interpreted	Out of 175 phrases which should be interpreted as semantic-role relation based on the dictionary, 13 were not analyzed correctly because of this type of problem.
discussion	Explanation for the findings	should be interpreted	However, a few comparisons with Aslin’s et al. (1996) data in Table 1 may be useful, although they should be interpreted cautiously given the differences in the training and testing corpora between their study and this one.
discussion	Suggestion of hypotheses	these findings suggest that	These findings suggest that it is possible to estimate the expected variation in durations of oral reading across texts.
discussion	Suggestion of hypotheses	these findings suggest that	These findings suggest that there is an inherent discrepancy between the assessment of the overall translation quality of a problem and the CAR.
discussion	Suggestion of hypotheses	these findings suggest that	These findings suggest that the target topic, having a known narrative frame, was the most useful piece of information for participants in ordering tweets.
discussion	Suggestion of hypotheses	these findings suggest that	These findings suggest that syntactic structure is a stronger determinant of gesture occurrence than theme or rheme and given or new information specified by local grammatical cues.
discussion	Suggestion of hypotheses	these findings suggest that	All these findings suggest that these three groups of metrics, the n-gram based metrics, the scene-graph based SPICE and the word embedding based WMD , can be complementary to each other.
discussion	Suggestion of hypotheses	we can infer that	From the ROUGE results in Table 6 we can infer that while REAPER outperformed ASRL on the query focused task, however it is notable that both systems under performed when compared to the top system from the DUC2006 conference.
discussion	Suggestion of hypotheses	we can infer that	Since we know that the total probability of all the frames containing e i must be greater than 0.5 in order for e i to be selected, we can infer that the total probability of all frames including e j must be less than 0.5, and thus that e j will not be selected.
discussion	Suggestion of hypotheses	support the hypothesis	These numbers support the hypothesis that our approach yields translations close to the reference wordforms but unjustly penalized by BLEU, which only gives credit for exact word matches 10 .
discussion	Suggestion of hypotheses	support the hypothesis	Therefore, the lack of these tags strongly support the hypothesis that the speakers in score group 1 showed incompetence in the use of relative clauses or their use in limited situations.
discussion	Suggestion of hypotheses	it can be hypothesized that	It can be hypothesized that this finding carries over to other treebanks that are annotated with flat structures.
discussion	Suggestion of hypotheses	it can be hypothesized that	From the results in Table 2, it can be hypothesized that Novelty is best captured at sentence scoring stage of summarization, rather than at ranking or summary extraction.
discussion	Suggestion of hypotheses	suggest that * exists	Second, our results suggest that there exists a clear relation between the two kinds of MT evaluation described.
discussion	Suggestion of hypotheses	suggest that * exists	These observations on stacksize and search errors suggest that there exists another maximization problem that is more suitable to summarization.
discussion	Suggestion of hypotheses	these results provide	Overall, these results provide suggestive evidence that both KSs are useful for learning-based coreference resolution.
discussion	Suggestion of hypotheses	these results provide	These results provide some insight as to why deep RNNs are able to model NLP tasks without annotated linguistic features.
discussion	Suggestion of hypotheses	these results provide	Hence, these results provide additional evidence in support of surprisal as a reliable measure of cognitive processing difficulty during sentence comprehension (CITE-p-19-6-5).
discussion	Implications of the findings	an implication of this	An implication of this property is that the system does not try to match the input utterance to the closest word (by some measure of distance) contained in the dictionary but rather tries to find its most probable spelling.
discussion	Implications of the findings	raises the possibility	Such suboptimality averagely balanced by the feature representation raises the possibility of achieving states that have a high-quality summary with a low score, since we do not have a genuine score function.
discussion	Implications of the findings	important implications	These results may have important implications for the design of support forums.
discussion	Implications of the findings	important implications	The capability of grounding semantic roles to the physical world has many important implications.
discussion	Implications of the findings	important implications	Automatic identification of user intent also has important implications in building intelligent conversational QA systems.
discussion	Implications of the findings	important implications	Nevertheless, the results from our current work have several important implications in building robust conversational interfaces.
discussion	Implications of the findings	important implications	Observations from our input-based evaluation also have important implications for the design of novel summarization tasks.
discussion	Implications of the findings	results raise	Such results raise the need for caution when using assessments for machine produced text to build a general model of fluency.
discussion	Implications of the findings	results raise	These results raise the methodological question of how to combine semantic, syntactic and lexical similarity measures in order to estimate the impact of the different strategies used on each dataset.
discussion	Comments on the findings	disappointing	On the one hand, the overall results are perhaps disappointing.
discussion	Comments on the findings	disappointing	This might seem disappointing considering the more complex machinery employed in our approach.
discussion	Comments on the findings	encouraging	Experimental results demonstrate the encouraging performance of the proposed approach.
discussion	Comments on the findings	encouraging	The experiment results based on our two assumptions are encouraging.
discussion	Comments on the findings	encouraging	Our empirical evaluations have shown encouraging results for both approaches.
discussion	Comments on the findings	encouraging	Moreover, the results obtained with those methods are quite encouraging.
discussion	Comments on the findings	was successful	By measuring inter-annotator agreement it shows that this process was successful.
discussion	Comments on the findings	was successful	Given that our method was successful, we anticipate that incorporating known diachronic biases will radically improve performance on natural language data.
discussion	Comments on the findings	was successful	Despite this, the model was successful as a language model for speech recognition, as measured by WER and ability to extract high-level information.
discussion	Comments on the findings	was successful	Thus our aim of evaluating our hybrid system against THYME corpus which was developed using Stanford TokensRegex Framework and Stanford CRF Classifier was successful.
discussion	Comments on the findings	was successful	Meanwhile, the studied discriminative boundary detection method based on the CRF framework was successful in gaining consistent reduction in all error types, given increasing amounts of data.
discussion	Comments on the findings	results are significant	We believe that these results are significant as well as insightful.
discussion	Comments on the findings	results are significant	The results are significant for the study of regular polysemy as the senses of many verbs readily divide into literal and metaphorical groups.
discussion	Comments on the findings	results are significant	Our approach and results are significant from several perspectives: they provide a strong approach to identifying posts indicating a risk of self-harm in social media; they demonstrate a means for large scale public mental health studies surrounding the state of depression; and they demonstrate the possibility of sensitive applications in the context of clinical care, where clinicians could be notified if the activities of their patients suggest they are at risk of self-harm.
discussion	Suggestion of future work	for future research	We believe this is another fruitful line for future research.
discussion	Suggestion of future work	for future research	This work leaves open several interesting avenues for future research.
discussion	Suggestion of future work	for future research	One interesting topic for future research is exploration in planning.
discussion	Suggestion of future work	there are still	However, there are still a number of avenues for improvement.
discussion	Suggestion of future work	there are still	There are still several open problems that should be investigated further:
discussion	Suggestion of future work	there are still	There are still several open problems that should be further investigated:
discussion	Suggestion of future work	there are still	There are still many challenges for generating high-quality event chronicles.
discussion	Suggestion of future work	there are still	Nevertheless, there are still a number of issues that need be addressed.
discussion	Suggestion of future work	questions remain	Some questions remain open to us regarding optimal size of training chat language corpus in XSCM.
discussion	Suggestion of future work	questions remain	As this is the first work which addressed unsupervised ARGID , many questions remain to be explored.
discussion	Suggestion of future work	questions remain	Since this paper reports about the first results from a new line of research, many questions remain open and demand furtherresearch.
discussion	Suggestion of future work	questions remain	This having said, some crucial questions remain before CMSMs can be applied in practice: How to acquire CMSMs for large token sets and specific purposes?
discussion	Suggestion of future work	questions remain	Many questions remain; in providing a quantitative description of this dataset, we hope to highlight its potential for analysis, and encourage other work in this domain.
discussion	Suggestion of future work	further work is required to	Additionally, further work is required to reduce the number of candidate relations recalled from the KB.
discussion	Suggestion of future work	further work is required to	Further work is required to understand what these interactions are and how they contribute to performance.
discussion	Suggestion of future work	further work is required to	Further work is required to reconcile our results with prior work on topic differences and audience size (CITE-p-12-3-2).
discussion	Suggestion of future work	further work is required to	While expanding the model to include more layers of similarity estimates is clearly a step in the right direction, further work is required to include even more layers.
discussion	Suggestion of future work	further work is required to	Some of the variation in system performance can be explained by the class imbalance present in the data sets for the different targets, but further work is required to identify other factors.
discussion	Suggestion of future work	for further progress	There are several potential avenues for further progress towards this goal, including the development of more portable SRL pipeline systems, and especially parsers.
discussion	Suggestion of future work	a further study	We would need a further study in the following points of view: First, we should evaluate our method with corpus in different domains.
discussion	Suggestion of future work	a further study	7 One explanation for this result could be that developers (or development tools) treat axioms as having a topic-comment structure, where the topic is usually the first argument; we intend to investigate this possibility in a further study.
discussion	Suggestion of future work	future studies	(Future studies will focus on a broader population of adults.)
discussion	Suggestion of future work	future studies	Future studies will also include experiments using data of various languages.
discussion	Suggestion of future work	future studies	First, we plan future studies to help expand our notion of visual salience.
discussion	Suggestion of future work	future studies	Future studies in early childhood readability need to take visual content into account.
discussion	Suggestion of future work	additional studies	Additional studies are being planned that will effectively test the Phrase Resolution Algorithm.
discussion	Suggestion of future work	additional studies	However, additional studies are needed to investigate why huge improvements in AER result in relatively smaller improvements in BLEU scores.
discussion	Suggestion of future work	additional studies	Since our method depends on title words and keywords, we need additional studies about the characteristics of candidate words for title words and keywords according to each data set.
