The aim of the paper (again)	Four human judges	'Four human judges were used to determine the ''gold standard''.'	A00-2034_s-4-1-1-3
Description of the results	The other 3	(The other 3 entrainment variables did not show significant relationships with participation.)	D16-1149_s-21-8-2-1
Description of the results	label balance	AUC scores are largely unaffected by change in label balance.	P16-3016_s-7-1-0-0
Description of the results	noticeable bias	Accordingly, we found no noticeable bias with regard to their numbers.	D12-1057_s-13-8-1-4
interesting or surprising results	WCNs	Although this might be counterintuitive, given that WCNs improve the slot accuracy overall.	E09-1028_s-11-11-3-4
The aim of the paper (again)	to evaluate our	Among them, 300 questions were used to train the Japanese answer extractor and 400 questions were used to evaluate our framework.	P07-1099_s-16-1-0-4
Reference to tables or figures	the table both translation directions	As can be seen from the table, both translation directions,	J06-4004_s-14-8-0-1
Reference to tables or figures	Average values for	Average values for each method are shown in Figure 1 and comparative values are shown in Figure 2.	N07-2044_s-5-1-0-1
Description of the results	are almost the	Both results are almost the same with no significant difference.	P18-1166_s-12-1-0-2
comparison of the results	had be underestimated by 013-018 percentage	By comparing the tagging accuracy for all words in tables 2 and 3, it can be seen that the accuracy had be underestimated by 0.13-0.18 percentage points.	E09-1060_s-12-1-4-2
Description of the results	even if dependency	By contrast, only a few inserted clauses were detected even if dependency structures were used.	P06-2042_s-9-5-1-0
comparison of the results	the score by about 20 points	Comparing the F1-macro with and without lexicon, it can be seen that the additional information stored in the lexicon strongly increases the score by about 20 points for English data.	N18-1134_s-12-1-1-3
Description of the results	training to some	Different initial values were shown to affect the results of training to some extent (Table 1).	P05-1010_s-12-1-0-4
Description of the results	up to 50	Discarding bottom ranked features (up to 50%), also, did not affect the results significantly.	P10-1095_s-12-3-2-1
Reference to tables or figures	sketch of	Figure 4 provides a rough sketch of the room layout.	J97-1006_s-14-1-0-0
Reference to tables or figures	more detailed	Figure 5 provides a more detailed characterization of LNQ’s performance.	P18-1029_s-12-6-1-0
Reference to tables or figures	the architecture	Figure 8 provides an overview of the architecture of MOOSE.	J98-3003_s-13-1-0-0
Description of the results	incorrectly classified and those that	First, there was a significant difference in the variability of judges’ responses between items that were incorrectly classified and those that were correct (66% vs. 73%, respectively; two-sample t test: MATH-w-15-9-2-88, MATH-w-15-9-2-98.02).	D13-1094_s-15-9-2-2
Description of the results	between Markov 07968 and	For the top three items, the difference between Markov (0.7968) and LSA (0.9965) was significant at the MATH-w-12-4-1-150 level.	N07-1060_s-12-4-1-4
Description of the results	due to our choice	Further analysis showed that its bad performance is due to our choice of confounders as highly similar lemmas (cf. Section 2).	N18-2033_s-5-1-1-2
Description of the results	Further testing	Further testing revealed that accuracy outside NEs was near 99%.	N04-2007_s-5-10-2-0
Description of the results	with a different	However, no significant differences were found between experiments with a different number of skips.	S14-2048_s-8-1-2-3
Description of the results	non-tailored ones in terms	However, there was no evidence that the tailored letters were any better than the non-tailored ones in terms of increasing cessation rates.	P01-1057_s-6-10-0-1
Description of the results	23 bursty topics	In the case of the LDA model, only 23 bursty topics were detected.	P12-1056_s-11-1-0-3
interesting or surprising results	different	Interestingly, ICA and ILP improve over Local in different ways.	D09-1018_s-13-4-3-0
interesting or surprising results	prior	Interestingly, all updates with Laplacian prior resulted in low performance.	P07-3009_s-11-8-0-2
Reference to tables or figures	that the lexical and gazetteer	It can be seen from Table 7 that the lexical and gazetteer related features are helpful.	P11-1037_s-21-1-0-3
Reference to tables or figures	performance in many genres	It is apparent from Table 2 that unigram features yield curiously high performance in many genres.	D13-1181_s-8-1-0-0
Description of the results	is thus	It is thus revealed that chat language is indeed dynamic.	P06-1125_s-19-1-2-4
interesting or surprising results	performance of	More surprising, however, is the mediocre performance of the WordNet lemmatizer.	Q16-1021_s-14-4-1-0
Description of the results	cases for which	No cases for which Recommendation 6 applies were found in the test corpus.	E17-3015_s-7-5-0-1
Description of the results	greater than 5	No significant improvement was observed in the results when the value of MPR was greater than 5.	N09-3016_s-7-3-1-1
Description of the results	for module by	No significant interaction effect was found for module by group.	P16-4021_s-6-1-1-7
Description of the results	statistically significant decrease	No statistically significant decrease in accuracy was detected in any of the algorithm/dataset combinations.	E12-1008_s-15-3-2-1
Description of the results	of induced descriptors is	None of the differences are statistically significant which lets us conclude that interpretability of induced descriptors is comparable for the RMN and MVPlot.	D17-1200_s-12-1-2-2
Description of the results	these changes	On the other hand, additive models remain unaffected by these changes.	P18-1012_s-12-1-0-1
interesting or surprising results	the object of	One of the most surprising results of our experiments is that the object of the action alone is a very effective feature, achieving 89.04%.	S14-1008_s-9-1-1-4
The aim of the paper (again)	Our experiments	Our experiments aimed to answer three questions: Which event chains are worth keeping?	E12-1034_s-8-1-0-0
Reference to tables or figures	curves are shown	Results are shown in the last row of Table 4 and precision-recall curves are shown in Figure 5.	N10-1119_s-9-17-1-1
Reference to tables or figures	of the data sets	Statistics of the data sets are summarized in Table 3.	D09-1114_s-7-1-0-5
interesting or surprising results	even	Surprisingly, GUSPEE even surpassed the supervised upper bound of MultiR.	N15-1077_s-15-1-1-2
interesting or surprising results	joint	Surprisingly, the joint models outperform slightly, yielding a 0.4% improvement.	D15-1040_s-15-9-2-1
Reference to tables or figures	transducers on	Table 1 compares different transducers on an English test case.	E97-1059_s-10-1-2-0
Reference to tables or figures	some statistics	Table 1 gives some examples, and Table 2 presents some statistics.	N07-2024_s-5-1-0-3
Reference to tables or figures	document statistics	Table 1 shows topic-length statistics, Table 2 shows document statistics, and Figure 4 shows an example topic.	E99-1013_s-10-1-1-0
Reference to tables or figures	Table 10	Table 10 shows results using unigrams, while Table 10 shows results for primary ciphers generated from bigrams.	D16-1141_s-15-1-0-1
Reference to tables or figures	and labels Eq	Table 2 summarizes the accuracy of various models on three datasets, revealing that our model offers competitive performance, both as a joint model of words and labels (Eq.	P18-1189_s-11-1-1-0
Reference to tables or figures	Table 4	Table 3 shows the BLEU scores, and Table 4 shows the precision.	N06-1001_s-14-1-0-1
Summary of the results	without simply rehashing the	Taken together these results indicate that the ILP models perform a fair amount of rewriting without simply rehashing the source sentence.	D11-1038_s-8-1-1-5
Summary of the results	a significant portion of	Taken together, these results are consistent with the idea that, at least for a significant portion of the data, the incorrect judgments made by both the judges and the model may have occurred on passages for which either including or omitting the connective would have been acceptable.	D13-1094_s-15-9-2-6
Summary of the results	its automatic variant is	Taken together, these results indicate that US IM , even in its automatic variant, is sensitive to semantic changes.	N18-2020_s-10-1-3-0
Summary of the results	for this type of	Taken together, these results indicate that the frequency of the construction is more important than the size of the training set for this type of transformation.	P07-1122_s-14-1-1-3
Summary of the results	combination contributes to the	Taken together, these results suggest that model combination contributes to the success of both models, but to a larger extent for RG.	P17-2025_s-7-4-2-1
Description of the results	is statistically	The MT06 result is statistically significant at MATH-w-15-6-0-22; MT08 is significant at p ≤ 0.02.	P12-1016_s-15-6-0-1
Reference to tables or figures	examples of polar	The Table 6 illustrates examples of polar phrases and their polarity values.	D07-1115_s-16-7-3-0
The aim of the paper (again)	The algorithm	The algorithm above was tested in the SILC translation system.	P96-1021_s-4-1-0-0
Description of the results	syntactic features	The analysis revealed that both structural and syntactic features are important.	N12-1009_s-24-1-0-1
Reference to tables or figures	experiments for Sieve using the lexical	The bottom half of the table shows cross-domain experiments for Sieve using the lexical heuristic at the end of its rule set (LexEnd).	P13-2015_s-10-6-1-1
Description of the results	dep1-L and	The comparison between dep1-L and dep1c-L is significant at p < 0.05, and all other comparisons are significant at p < 0.0005.	P08-1068_s-10-5-0-1
The aim of the paper (again)	extracted 10	The extracted 10 words were compared with the manually labeled keywords.	P07-1070_s-18-1-1-3
The aim of the paper (again)	in the	The grammatical channel was tested in the SILC translation system.	P98-2230_s-10-1-0-0
The aim of the paper (again)	The hardware	The hardware was tested with 1 core and 5 cores.	P15-2063_s-10-4-0-1
Description of the results	those with extensive formal training	The mean score for those with extensive formal training was 90.9%.	N18-1175_s-13-1-1-4
interesting or surprising results	the tremendous efficiency	The most striking fact about this graph is the tremendous efficiency of the extension model.	P95-1030_s-8-6-0-1
interesting or surprising results	performance was achieved	The most surprising finding is that the best performance was achieved by the unlexicalized PCFG baseline model.	P03-1013_s-13-1-2-0
The aim of the paper (again)	    	The purpose of creating this subset model was to simulate a resource-poor language.	D09-1040_s-7-1-0-4
The aim of the paper (again)	any patterns consistently produce poor	The purpose of this evaluation was to determine if any patterns consistently produce poor questions.	P14-2053_s-9-1-0-1
The aim of the paper (again)	    	The purpose of this stage was to correct obvious mistakes.	P13-1069_s-5-6-0-4
The aim of the paper (again)	possible differences between WSI of	The purpose of this test was to demonstrate possible differences between WSI of different word class combinations.	E06-1018_s-6-6-0-2
Reference to tables or figures	6 and 7	The results are presented in Tables 5, 6, and 7.	D17-1204_s-13-1-0-0
interesting or surprising results	pair were	The results for the more challenging English-Chinese pair were more surprising.	D15-1128_s-10-1-0-3
Description of the results	the sciences and humanities	The results indicate that the sciences and humanities share several topics.	D15-1179_s-16-1-1-0
Description of the results	outperforms all the baselines	The results indicate that time-aware embedding outperforms all the baselines consistently.	D16-1260_s-9-1-3-2
Reference to tables or figures	results with this setting	The results with this setting are summarized in Table 5.	D07-1033_s-12-1-4-2
Reference to tables or figures	Cannot-link tuple rules	The second half of the table illustrates Cannot-link tuple rules.	D13-1040_s-10-6-2-5
Reference to tables or figures	significance levels of	The significance levels of the improvements over Local are highlighted in Table 4.	D09-1018_s-13-4-2-5
interesting or surprising results	Chicago Bears	The sixth row is interesting because Dave Toub is indeed affiliated with the Chicago Bears .	P12-1072_s-15-8-2-7
Reference to tables or figures	for this data	The statistics for this data are presented in Table 1.	D08-1023_s-8-1-0-1
Reference to tables or figures	The statistics of the	The statistics of the datasets are summarized in Table 1.	D16-1171_s-10-1-0-1
Reference to tables or figures	test data only	The table also illustrates three baseline results on the test data only.	D08-1030_s-13-3-1-3
Reference to tables or figures	shows the results on the English	The top half of the table shows the results on the English evaluation set.	P17-1009_s-23-1-0-1
The aim of the paper (again)	to generate the	The validation set was used to estimate performance during algorithm development and the test set was used to generate the final results.	D12-1069_s-19-1-1-6
The aim of the paper (again)	set was used	The validation set was used to estimate performance during algorithm development, while the test set was used to generate the final experimental results.	D12-1069_s-17-1-0-6
Description of the results	and C1 models	There is a significant increase between “C:±2” and “C:±1” models.	P11-1139_s-17-1-0-5
Description of the results	in performance across different relations	There was a significant difference in performance across different relations.	S07-1082_s-7-1-0-1
Description of the results	learning gain between	There was no significant difference in learning gain between conditions.	P10-2009_s-4-1-1-1
Description of the results	between the F1	There was positive correlation (MATH-w-9-1-3-33) between the F1 score for a given class and the raw number of training examples representing that class.	S16-1074_s-9-1-3-1
Description of the results	score line 5	These features cause no significant difference in score (line 5).	D09-1076_s-11-6-4-3
Description of the results	especially for short	These metrics were shown to perform well for single document text summarization, especially for short summaries.	S14-1010_s-14-1-0-4
Summary of the results	leveraging unlabeled data yields	These results indicate that leveraging unlabeled data yields significant improvements.	S14-2120_s-13-7-3-1
Summary of the results	responses with higher quality	These results indicate that our MGL generates responses with higher quality.	P18-1137_s-14-5-0-5
Summary of the results	topic modeler separates topics	These results indicate that the topic modeler separates topics very efficiently.	E17-1033_s-16-1-0-2
Summary of the results	well with both CFG	These results show that PRINCIPAR compares quite well with both CFG parsers.	J95-2005_s-12-6-1-3
Summary of the results	stage 2 is central	These results show that stage 2 is central to the aligner’s success.	D15-1111_s-16-1-1-4
Summary of the results	the LexHMM is modelling	These results show that the LexHMM is modelling ambiguity classes as intended.	E14-1013_s-10-4-2-2
Summary of the results	to improve translation quality	These results suggest that rewriting is estimated to improve translation quality.	P13-4015_s-7-4-1-0
Description of the results	to English results	This did not affect to English results because of the spareness of “vague” links.	S10-1063_s-18-11-1-6
Reference to tables or figures	where row B-opt-F	This is highlighted in Table 2, where row “B-opt-F” contains the best results optimizing the entity F-measure (at MATH-w-12-11-1-73), row “B-opt-AV” contains the best results optimizing ACE-Value (at MATH-w-12-11-1-93), and the last line “Twin-model” contains the results of the proposed twin-model.	N07-1010_s-12-11-1-1
Reference to tables or figures	This is indeed the case	This is indeed the case, as can be seen from Figure 6.	P11-1028_s-10-8-2-3
interesting or surprising results	This is	This is interesting because these two SE-types constitute the broader SE category of generalizing statives.	P07-1113_s-20-1-1-1
Reference to tables or figures	not always true	This is not always true, as shown in Experiment I.	D07-1066_s-11-1-0-5
interesting or surprising results	very	This is very counterintuitive, and we are still trying hard to find its real cause.	S18-1176_s-17-1-0-5
The aim of the paper (again)	the test word	Trigram models were used to compute the test word perplexity.	J09-1002_s-17-4-0-2
The aim of the paper (again)	92 most	We aimed to extract facts of the 92 most frequent relations in Freebase 2009.	P13-2141_s-9-1-0-0
The aim of the paper (again)	compatible schemes	We aimed to include as many different domains as possible annotated under compatible schemes.	N10-1004_s-10-1-0-0
Description of the results	performance particularly much	We allowed our model to tune on the gold data, which surprisingly did not increase performance particularly much.	S17-2021_s-12-1-0-4
Description of the results	similarity between Romanian	We also notice a significant increase in similarity between Romanian and English in the modern period.	D14-1112_s-14-1-1-7
Description of the results	alternative was	We found no significant improvement when the alternative was used.	N04-1010_s-8-17-2-3
Reference to tables or figures	results are similar	We see from this table that the results are similar across the two tasks.	P12-1063_s-19-1-0-3
comparison of the results	MIRA outperforms the averaged perceptron method	When comparing the two online learning models, it can be seen that MIRA outperforms the averaged perceptron method.	P05-1012_s-10-5-1-6
Description of the results	component incorrectly classified	Where suitable evidence was found, the RTE component incorrectly classified 13.84% (MATH-w-22-1-0-117) of claims.	N18-1074_s-22-1-0-3
