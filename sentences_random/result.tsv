The aim of the paper (again)	determine the gold standard	'Four human judges were used to determine the ''gold standard''.'	A00-2034_s-4-1-1-3
Description of the results	significant relationships with participation	(The other 3 entrainment variables did not show significant relationships with participation.)	D16-1149_s-21-8-2-1
Description of the results	AUC scores are largely	AUC scores are largely unaffected by change in label balance.	P16-3016_s-7-1-0-0
Description of the results	noticeable bias with regard	Accordingly, we found no noticeable bias with regard to their numbers.	D12-1057_s-13-8-1-4
interesting or surprising results	Although this might be	Although this might be counterintuitive, given that WCNs improve the slot accuracy overall.	E09-1028_s-11-11-3-4
The aim of the paper (again)	Among them 300 questions	Among them, 300 questions were used to train the Japanese answer extractor and 400 questions were used to evaluate our framework.	P07-1099_s-16-1-0-4
Reference to tables or figures	the table both translation	As can be seen from the table, both translation directions,	J06-4004_s-14-8-0-1
Reference to tables or figures	shown in Figure 2	Average values for each method are shown in Figure 1 and comparative values are shown in Figure 2.	N07-2044_s-5-1-0-1
Description of the results	Both results are almost	Both results are almost the same with no significant difference.	P18-1166_s-12-1-0-2
comparison of the results	the accuracy had be	By comparing the tagging accuracy for all words in tables 2 and 3, it can be seen that the accuracy had be underestimated by 0.13-0.18 percentage points.	E09-1060_s-12-1-4-2
Description of the results	if dependency structures were	By contrast, only a few inserted clauses were detected even if dependency structures were used.	P06-2042_s-9-5-1-0
comparison of the results	lexicon strongly increases the	Comparing the F1-macro with and without lexicon, it can be seen that the additional information stored in the lexicon strongly increases the score by about 20 points for English data.	N18-1134_s-12-1-1-3
Description of the results	some extent Table 1	Different initial values were shown to affect the results of training to some extent (Table 1).	P05-1010_s-12-1-0-4
Description of the results	up to 50 also	Discarding bottom ranked features (up to 50%), also, did not affect the results significantly.	P10-1095_s-12-3-2-1
Reference to tables or figures	sketch of the room	Figure 4 provides a rough sketch of the room layout.	J97-1006_s-14-1-0-0
Reference to tables or figures	detailed characterization of LNQs	Figure 5 provides a more detailed characterization of LNQ’s performance.	P18-1029_s-12-6-1-0
Reference to tables or figures	overview of the architecture	Figure 8 provides an overview of the architecture of MOOSE.	J98-3003_s-13-1-0-0
Description of the results	were incorrectly classified and	First, there was a significant difference in the variability of judges’ responses between items that were incorrectly classified and those that were correct (66% vs. 73%, respectively; two-sample t test: MATH-w-15-9-2-88, MATH-w-15-9-2-98.02).	D13-1094_s-15-9-2-2
Description of the results	Markov 07968 and LSA	For the top three items, the difference between Markov (0.7968) and LSA (0.9965) was significant at the MATH-w-12-4-1-150 level.	N07-1060_s-12-4-1-4
Description of the results	its bad performance is	Further analysis showed that its bad performance is due to our choice of confounders as highly similar lemmas (cf. Section 2).	N18-2033_s-5-1-1-2
Description of the results	outside NEs was near	Further testing revealed that accuracy outside NEs was near 99%.	N04-2007_s-5-10-2-0
Description of the results	different number of skips	However, no significant differences were found between experiments with a different number of skips.	S14-2048_s-8-1-2-3
Description of the results	of increasing cessation rates	However, there was no evidence that the tailored letters were any better than the non-tailored ones in terms of increasing cessation rates.	P01-1057_s-6-10-0-1
Description of the results	case of the LDA	In the case of the LDA model, only 23 bursty topics were detected.	P12-1056_s-11-1-0-3
interesting or surprising results	ILP improve over Local	Interestingly, ICA and ILP improve over Local in different ways.	D09-1018_s-13-4-3-0
interesting or surprising results	prior resulted in low	Interestingly, all updates with Laplacian prior resulted in low performance.	P07-3009_s-11-8-0-2
Reference to tables or figures	and gazetteer related features	It can be seen from Table 7 that the lexical and gazetteer related features are helpful.	P11-1037_s-21-1-0-3
Reference to tables or figures	2 that unigram features	It is apparent from Table 2 that unigram features yield curiously high performance in many genres.	D13-1181_s-8-1-0-0
Description of the results	language is indeed dynamic	It is thus revealed that chat language is indeed dynamic.	P06-1125_s-19-1-2-4
interesting or surprising results	is the mediocre performance	More surprising, however, is the mediocre performance of the WordNet lemmatizer.	Q16-1021_s-14-4-1-0
Description of the results	in the test corpus	No cases for which Recommendation 6 applies were found in the test corpus.	E17-3015_s-7-5-0-1
Description of the results	when the value of	No significant improvement was observed in the results when the value of MPR was greater than 5.	N09-3016_s-7-3-1-1
Description of the results	for module by group	No significant interaction effect was found for module by group.	P16-4021_s-6-1-1-7
Description of the results	of the algorithmdataset combinations	No statistically significant decrease in accuracy was detected in any of the algorithm/dataset combinations.	E12-1008_s-15-3-2-1
Description of the results	lets us conclude that	None of the differences are statistically significant which lets us conclude that interpretability of induced descriptors is comparable for the RMN and MVPlot.	D17-1200_s-12-1-2-2
Description of the results	On the other hand	On the other hand, additive models remain unaffected by these changes.	P18-1012_s-12-1-0-1
interesting or surprising results	experiments is that the	One of the most surprising results of our experiments is that the object of the action alone is a very effective feature, achieving 89.04%.	S14-1008_s-9-1-1-4
The aim of the paper (again)	event chains are worth	Our experiments aimed to answer three questions: Which event chains are worth keeping?	E12-1034_s-8-1-0-0
Reference to tables or figures	of Table 4 and	Results are shown in the last row of Table 4 and precision-recall curves are shown in Figure 5.	N10-1119_s-9-17-1-1
Reference to tables or figures	Statistics of the data	Statistics of the data sets are summarized in Table 3.	D09-1114_s-7-1-0-5
interesting or surprising results	supervised upper bound of	Surprisingly, GUSPEE even surpassed the supervised upper bound of MultiR.	N15-1077_s-15-1-1-2
interesting or surprising results	slightly yielding a 04	Surprisingly, the joint models outperform slightly, yielding a 0.4% improvement.	D15-1040_s-15-9-2-1
Reference to tables or figures	on an English test	Table 1 compares different transducers on an English test case.	E97-1059_s-10-1-2-0
Reference to tables or figures	some examples and Table	Table 1 gives some examples, and Table 2 presents some statistics.	N07-2024_s-5-1-0-3
Reference to tables or figures	document statistics and Figure	Table 1 shows topic-length statistics, Table 2 shows document statistics, and Figure 4 shows an example topic.	E99-1013_s-10-1-1-0
Reference to tables or figures	while Table 10 shows	Table 10 shows results using unigrams, while Table 10 shows results for primary ciphers generated from bigrams.	D16-1141_s-15-1-0-1
Reference to tables or figures	accuracy of various models	Table 2 summarizes the accuracy of various models on three datasets, revealing that our model offers competitive performance, both as a joint model of words and labels (Eq.	P18-1189_s-11-1-1-0
Reference to tables or figures	scores and Table 4	Table 3 shows the BLEU scores, and Table 4 shows the precision.	N06-1001_s-14-1-0-1
Summary of the results	that the ILP models	Taken together these results indicate that the ILP models perform a fair amount of rewriting without simply rehashing the source sentence.	D11-1038_s-8-1-1-5
Summary of the results	are consistent with the	Taken together, these results are consistent with the idea that, at least for a significant portion of the data, the incorrect judgments made by both the judges and the model may have occurred on passages for which either including or omitting the connective would have been acceptable.	D13-1094_s-15-9-2-6
Summary of the results	its automatic variant is	Taken together, these results indicate that US IM , even in its automatic variant, is sensitive to semantic changes.	N18-2020_s-10-1-3-0
Summary of the results	is more important than	Taken together, these results indicate that the frequency of the construction is more important than the size of the training set for this type of transformation.	P07-1122_s-14-1-1-3
Summary of the results	combination contributes to the	Taken together, these results suggest that model combination contributes to the success of both models, but to a larger extent for RG.	P17-2025_s-7-4-2-1
Description of the results	is significant at p	The MT06 result is statistically significant at MATH-w-15-6-0-22; MT08 is significant at p ≤ 0.02.	P12-1016_s-15-6-0-1
Reference to tables or figures	and their polarity values	The Table 6 illustrates examples of polar phrases and their polarity values.	D07-1115_s-16-7-3-0
The aim of the paper (again)	the SILC translation system	The algorithm above was tested in the SILC translation system.	P96-1021_s-4-1-0-0
Description of the results	and syntactic features are	The analysis revealed that both structural and syntactic features are important.	N12-1009_s-24-1-0-1
Reference to tables or figures	for Sieve using the	The bottom half of the table shows cross-domain experiments for Sieve using the lexical heuristic at the end of its rule set (LexEnd).	P13-2015_s-10-6-1-1
Description of the results	comparisons are significant at	The comparison between dep1-L and dep1c-L is significant at p < 0.05, and all other comparisons are significant at p < 0.0005.	P08-1068_s-10-5-0-1
The aim of the paper (again)	The extracted 10 words	The extracted 10 words were compared with the manually labeled keywords.	P07-1070_s-18-1-1-3
The aim of the paper (again)	in the SILC translation	The grammatical channel was tested in the SILC translation system.	P98-2230_s-10-1-0-0
The aim of the paper (again)	1 core and 5	The hardware was tested with 1 core and 5 cores.	P15-2063_s-10-4-0-1
Description of the results	with extensive formal training	The mean score for those with extensive formal training was 90.9%.	N18-1175_s-13-1-1-4
interesting or surprising results	this graph is the	The most striking fact about this graph is the tremendous efficiency of the extension model.	P95-1030_s-8-6-0-1
interesting or surprising results	was achieved by the	The most surprising finding is that the best performance was achieved by the unlexicalized PCFG baseline model.	P03-1013_s-13-1-2-0
The aim of the paper (again)	creating this subset model	The purpose of creating this subset model was to simulate a resource-poor language.	D09-1040_s-7-1-0-4
The aim of the paper (again)	if any patterns consistently	The purpose of this evaluation was to determine if any patterns consistently produce poor questions.	P14-2053_s-9-1-0-1
The aim of the paper (again)	   	The purpose of this stage was to correct obvious mistakes.	P13-1069_s-5-6-0-4
The aim of the paper (again)	between WSI of different	The purpose of this test was to demonstrate possible differences between WSI of different word class combinations.	E06-1018_s-6-6-0-2
Reference to tables or figures	Tables 5 6 and	The results are presented in Tables 5, 6, and 7.	D17-1204_s-13-1-0-0
interesting or surprising results	challenging English-Chinese pair were	The results for the more challenging English-Chinese pair were more surprising.	D15-1128_s-10-1-0-3
Description of the results	the sciences and humanities	The results indicate that the sciences and humanities share several topics.	D15-1179_s-16-1-1-0
Description of the results	time-aware embedding outperforms all	The results indicate that time-aware embedding outperforms all the baselines consistently.	D16-1260_s-9-1-3-2
Reference to tables or figures	The results with this	The results with this setting are summarized in Table 5.	D07-1033_s-12-1-4-2
Reference to tables or figures	The second half of	The second half of the table illustrates Cannot-link tuple rules.	D13-1040_s-10-6-2-5
Reference to tables or figures	improvements over Local are	The significance levels of the improvements over Local are highlighted in Table 4.	D09-1018_s-13-4-2-5
interesting or surprising results	The sixth row is	The sixth row is interesting because Dave Toub is indeed affiliated with the Chicago Bears .	P12-1072_s-15-8-2-7
Reference to tables or figures	statistics for this data	The statistics for this data are presented in Table 1.	D08-1023_s-8-1-0-1
Reference to tables or figures	The statistics of the	The statistics of the datasets are summarized in Table 1.	D16-1171_s-10-1-0-1
Reference to tables or figures	on the test data	The table also illustrates three baseline results on the test data only.	D08-1030_s-13-3-1-3
Reference to tables or figures	shows the results on	The top half of the table shows the results on the English evaluation set.	P17-1009_s-23-1-0-1
The aim of the paper (again)	development and the test	The validation set was used to estimate performance during algorithm development and the test set was used to generate the final results.	D12-1069_s-19-1-1-6
The aim of the paper (again)	to generate the final	The validation set was used to estimate performance during algorithm development, while the test set was used to generate the final experimental results.	D12-1069_s-17-1-0-6
Description of the results	between C2 and C1	There is a significant increase between “C:±2” and “C:±1” models.	P11-1139_s-17-1-0-5
Description of the results	in performance across different	There was a significant difference in performance across different relations.	S07-1082_s-7-1-0-1
Description of the results	in learning gain between	There was no significant difference in learning gain between conditions.	P10-2009_s-4-1-1-1
Description of the results	examples representing that class	There was positive correlation (MATH-w-9-1-3-33) between the F1 score for a given class and the raw number of training examples representing that class.	S16-1074_s-9-1-3-1
Description of the results	in score line 5	These features cause no significant difference in score (line 5).	D09-1076_s-11-6-4-3
Description of the results	single document text summarization	These metrics were shown to perform well for single document text summarization, especially for short summaries.	S14-1010_s-14-1-0-4
Summary of the results	unlabeled data yields significant	These results indicate that leveraging unlabeled data yields significant improvements.	S14-2120_s-13-7-3-1
Summary of the results	generates responses with higher	These results indicate that our MGL generates responses with higher quality.	P18-1137_s-14-5-0-5
Summary of the results	the topic modeler separates	These results indicate that the topic modeler separates topics very efficiently.	E17-1033_s-16-1-0-2
Summary of the results	quite well with both	These results show that PRINCIPAR compares quite well with both CFG parsers.	J95-2005_s-12-6-1-3
Summary of the results	stage 2 is central	These results show that stage 2 is central to the aligner’s success.	D15-1111_s-16-1-1-4
Summary of the results	modelling ambiguity classes as	These results show that the LexHMM is modelling ambiguity classes as intended.	E14-1013_s-10-4-2-2
Summary of the results	is estimated to improve	These results suggest that rewriting is estimated to improve translation quality.	P13-4015_s-7-4-1-0
Description of the results	of the spareness of	This did not affect to English results because of the spareness of “vague” links.	S10-1063_s-18-11-1-6
Reference to tables or figures	2 where row B-opt-F	This is highlighted in Table 2, where row “B-opt-F” contains the best results optimizing the entity F-measure (at MATH-w-12-11-1-73), row “B-opt-AV” contains the best results optimizing ACE-Value (at MATH-w-12-11-1-93), and the last line “Twin-model” contains the results of the proposed twin-model.	N07-1010_s-12-11-1-1
Reference to tables or figures	This is indeed the	This is indeed the case, as can be seen from Figure 6.	P11-1028_s-10-8-2-3
interesting or surprising results	these two SE-types constitute	This is interesting because these two SE-types constitute the broader SE category of generalizing statives.	P07-1113_s-20-1-1-1
Reference to tables or figures	is not always true	This is not always true, as shown in Experiment I.	D07-1066_s-11-1-0-5
interesting or surprising results	trying hard to find	This is very counterintuitive, and we are still trying hard to find its real cause.	S18-1176_s-17-1-0-5
The aim of the paper (again)	the test word perplexity	Trigram models were used to compute the test word perplexity.	J09-1002_s-17-4-0-2
The aim of the paper (again)	relations in Freebase 2009	We aimed to extract facts of the 92 most frequent relations in Freebase 2009.	P13-2141_s-9-1-0-0
The aim of the paper (again)	different domains as possible	We aimed to include as many different domains as possible annotated under compatible schemes.	N10-1004_s-10-1-0-0
Description of the results	tune on the gold	We allowed our model to tune on the gold data, which surprisingly did not increase performance particularly much.	S17-2021_s-12-1-0-4
Description of the results	English in the modern	We also notice a significant increase in similarity between Romanian and English in the modern period.	D14-1112_s-14-1-1-7
Description of the results	improvement when the alternative	We found no significant improvement when the alternative was used.	N04-1010_s-8-17-2-3
Reference to tables or figures	the results are similar	We see from this table that the results are similar across the two tasks.	P12-1063_s-19-1-0-3
comparison of the results	two online learning models	When comparing the two online learning models, it can be seen that MIRA outperforms the averaged perceptron method.	P05-1012_s-10-5-1-6
Description of the results	RTE component incorrectly classified	Where suitable evidence was found, the RTE component incorrectly classified 13.84% (MATH-w-22-1-0-117) of claims.	N18-1074_s-22-1-0-3
